{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9258064,"sourceType":"datasetVersion","datasetId":5601548}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Self-Supervised Contrastive Deep Learning with SimCLR\n\n### SimCLR: a Simple framework for Contrastive Learning of visual Representations","metadata":{}},{"cell_type":"markdown","source":"\n\n\n**Self-supervised:** since it relies on nonhuman-made labels in the pre-training phase,\n\n**Contrastive:** since the concept is to compare/contrast between input samples and distinguish similar from non-similar,\n\n**Deep:** since a deep nerural network is utilized for the learning,\n\n\n\n**Overview:**\n\nIn terms of money, time and effort, it's expensive to produce ground truth labels for the entire collected dataset. \nThis phenomena is widely known as label-scarcity and can hinder machine learning enigeers and data scientists \nfrom building models with considerable accuracy due to small-scale ground thruth at their disposal.\n\nSelf-supervised learning is a novel learning concept that is gaining fame in research in the recent years.\nThe priciple to pre-train then fine-tune. \nIn situations where only a small portion of the collected dataset is annotated while the larger portion is left unannotated,\nself-supervised learning offers a new dimension.\nBriefly, the steps:\n\n    1. Split the dataset into 1) labeled subset, and 2) unlabled subset,\n    \n    2.1 Use the unlabeled subset in a pre-training phase, train a model according to some critieria (e.g. SimCLR),\n    \n    2.2 Produce labels automatically using an augmentation module.\n    \n    3. Save and copy the optimal parameters obtained through the pre-training phase,\n   \n    4. Perform transfer learning,\n    \n    5. Retrain a new model on the labeled (ground truth) subset with the help of parameters obtained previously\n    \nAmongst many pre-training frameworks, SimCLR is one of the most prominent examples.\n\n\n\n**Dataset:**\n\nWe use a garbage collection image dataset that contains 4,661 labeled images of different classes (battery, bio, clothes, metal, paper, plastic, ... ),\nand 10,854 unlabeled images of the same classes.","metadata":{}},{"cell_type":"markdown","source":"## First Experiment:\n### Train a supervised classifier on the labeled dataset only.","metadata":{}},{"cell_type":"markdown","source":"### 1. Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:58:17.088812Z","iopub.execute_input":"2024-08-27T12:58:17.089246Z","iopub.status.idle":"2024-08-27T12:58:21.916141Z","shell.execute_reply.started":"2024-08-27T12:58:17.089204Z","shell.execute_reply":"2024-08-27T12:58:21.915176Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 2. Load and prepare the dataset","metadata":{}},{"cell_type":"code","source":"# Define data transforms (adjust as needed for grayscale images)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to match model input size\n    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n])\n\n# Load the dataset\npath_to_data = '/kaggle/input/garbage-collection/Garbage Labeled/Garbage Labeled'\nall_dataset = datasets.ImageFolder(root=path_to_data, transform=transform)\n\n# Define the split ratio\ntrain_ratio = 0.7\ntest_ratio = 0.3\ntotal_size = len(all_dataset)\ntrain_size = int(train_ratio * total_size)\ntest_size = total_size - train_size\n\n# Split the dataset\ntrain_dataset, test_dataset = random_split(all_dataset, [train_size, test_size])\n\n# Create DataLoaders for training and testing\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:58:30.740309Z","iopub.execute_input":"2024-08-27T12:58:30.741150Z","iopub.status.idle":"2024-08-27T12:58:33.516401Z","shell.execute_reply.started":"2024-08-27T12:58:30.741110Z","shell.execute_reply":"2024-08-27T12:58:33.515596Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 3. Construct Model (CNN): a Residual Network of 18 layers.","metadata":{}},{"cell_type":"code","source":"# Load  ResNet-18\nmodel_notPretrained = models.resnet18(pretrained=False) \n# Get the number of input features for the last fully connected layer\nnum_features = model_notPretrained.fc.in_features  \n# Modify the output layer for your specific problem\nmodel_notPretrained.fc = nn.Linear(num_features, 12)  ","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:07.415482Z","iopub.execute_input":"2024-08-27T12:59:07.415872Z","iopub.status.idle":"2024-08-27T12:59:07.649949Z","shell.execute_reply.started":"2024-08-27T12:59:07.415834Z","shell.execute_reply":"2024-08-27T12:59:07.649147Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4. Training Loop:","metadata":{}},{"cell_type":"code","source":"## %%time\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_notPretrained.parameters(), lr=0.001)\n\nnum_epochs = 20  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint('Device: ', device)\nmodel_notPretrained.to(device)\n\nfor epoch in range(num_epochs):\n    model_notPretrained.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model_notPretrained(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_loader)}\")\n\nprint(\"Training completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:59:28.285688Z","iopub.execute_input":"2024-08-27T12:59:28.286294Z","iopub.status.idle":"2024-08-27T13:07:22.157543Z","shell.execute_reply.started":"2024-08-27T12:59:28.286254Z","shell.execute_reply":"2024-08-27T13:07:22.156687Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Device:  cuda\nEpoch [1/20] Loss: 1.839085805649851\nEpoch [2/20] Loss: 1.5969772759605856\nEpoch [3/20] Loss: 1.482143078364578\nEpoch [4/20] Loss: 1.3593986501880722\nEpoch [5/20] Loss: 1.3123501007463418\nEpoch [6/20] Loss: 1.2593569796459347\nEpoch [7/20] Loss: 1.1642034398574455\nEpoch [8/20] Loss: 1.1042927176344628\nEpoch [9/20] Loss: 1.02481443975486\nEpoch [10/20] Loss: 0.9450821318462783\nEpoch [11/20] Loss: 0.879255511597091\nEpoch [12/20] Loss: 0.8098385836563858\nEpoch [13/20] Loss: 0.7338304326814764\nEpoch [14/20] Loss: 0.6454079872837254\nEpoch [15/20] Loss: 0.596867490048502\nEpoch [16/20] Loss: 0.48953332357546864\nEpoch [17/20] Loss: 0.35638870627564545\nEpoch [18/20] Loss: 0.28485892683851954\nEpoch [19/20] Loss: 0.2668719331131262\nEpoch [20/20] Loss: 0.1911693909662027\nTraining completed.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 5. Testing Loop:","metadata":{}},{"cell_type":"code","source":"model_notPretrained.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model_notPretrained(inputs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\naccuracy_before_transfer_learning = np.mean(np.array(all_preds) == np.array(all_labels))\nconf_matrix = confusion_matrix(all_labels, all_preds)\nclass_report = classification_report(all_labels, all_preds)\n\nprint(f\"Accuracy: {accuracy_before_transfer_learning:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:12:24.842224Z","iopub.execute_input":"2024-08-27T13:12:24.843202Z","iopub.status.idle":"2024-08-27T13:12:44.824418Z","shell.execute_reply.started":"2024-08-27T13:12:24.843153Z","shell.execute_reply":"2024-08-27T13:12:44.823323Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Accuracy: 0.5054\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Second Experiment: \n### Train an unsupervised model using SimCLR on the unlabeled dataset, \n###  then perform transfer learning, and retrain the model on the labeled dataset.","metadata":{}},{"cell_type":"markdown","source":"### 1. Import necessary libraries","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:44:37.596856Z","iopub.execute_input":"2024-08-27T12:44:37.597302Z","iopub.status.idle":"2024-08-27T12:44:37.601589Z","shell.execute_reply.started":"2024-08-27T12:44:37.597250Z","shell.execute_reply":"2024-08-27T12:44:37.600692Z"}}},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torchvision\nfrom PIL import Image\nfrom sklearn.preprocessing import normalize\nfrom lightly.data import LightlyDataset\nfrom lightly.transforms import SimCLRTransform, utils\n\n# you might need to install lighty, simply run: !pip install lightly ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-27T13:15:05.403916Z","iopub.execute_input":"2024-08-27T13:15:05.404323Z","iopub.status.idle":"2024-08-27T13:15:06.815846Z","shell.execute_reply.started":"2024-08-27T13:15:05.404282Z","shell.execute_reply":"2024-08-27T13:15:06.814248Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 2. Necessary configurations","metadata":{}},{"cell_type":"code","source":"num_workers = 8\nbatch_size = 256\nseed = 1\nmax_epochs = 20\ninput_size = 224\nnum_ftrs = 32\npl.seed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:15:18.169794Z","iopub.execute_input":"2024-08-27T13:15:18.170193Z","iopub.status.idle":"2024-08-27T13:15:18.186599Z","shell.execute_reply.started":"2024-08-27T13:15:18.170153Z","shell.execute_reply":"2024-08-27T13:15:18.185860Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. Load and prepare unlabeled dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:08:40.067886Z","iopub.execute_input":"2024-08-27T11:08:40.068604Z","iopub.status.idle":"2024-08-27T11:08:40.072777Z","shell.execute_reply.started":"2024-08-27T11:08:40.068559Z","shell.execute_reply":"2024-08-27T11:08:40.071802Z"}}},{"cell_type":"code","source":"path_to_data = '/kaggle/input/garbage-collection/Garbage Unlabeled/Garbage Unlabeled'\n\n# The following transform will return two augmented images per input image.\ntransform = SimCLRTransform(input_size=input_size, vf_prob=0.5, rr_prob=0.5)\n# vf_prob: Probability that vertical flip is applied.\n# rr_prob: Probability that random rotation is applied.\n\n# We create a torchvision transformation for embedding the dataset after training\ntest_transform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((input_size, input_size)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(\n            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n            std=utils.IMAGENET_NORMALIZE[\"std\"],\n        ),\n    ]\n)\n\n\ndataset_train_simclr = LightlyDataset(input_dir=path_to_data, transform=transform)\n\ndataloader_train_simclr = torch.utils.data.DataLoader(\n    dataset_train_simclr,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=True,\n    num_workers=num_workers,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:15:23.400456Z","iopub.execute_input":"2024-08-27T13:15:23.401440Z","iopub.status.idle":"2024-08-27T13:15:37.917471Z","shell.execute_reply.started":"2024-08-27T13:15:23.401396Z","shell.execute_reply":"2024-08-27T13:15:37.916470Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 4. Construct SimCLR Model:","metadata":{}},{"cell_type":"code","source":"from lightly.loss import NTXentLoss\nfrom lightly.models.modules.heads import SimCLRProjectionHead\n\n\nclass SimCLRModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        # create a ResNet backbone and remove the classification head\n        resnet = torchvision.models.resnet18()\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n        hidden_dim = resnet.fc.in_features\n        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n        \n        # use a criterion for self-supervised learning # (normalized temperature-scaled cross entropy loss)\n        self.criterion = NTXentLoss(temperature=0.5)\n\n    def forward(self, x):\n        h = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(h)\n        return z\n\n    def training_step(self, batch, batch_idx):\n        (x0, x1), _, _ = batch\n        z0 = self.forward(x0)\n        z1 = self.forward(x1)\n        loss = self.criterion(z0, z1)\n        self.log(\"train_loss_ssl\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        # get a PyTorch optimizer\n        optim = torch.optim.SGD(self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n        return [optim], [scheduler]","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:15:57.479372Z","iopub.execute_input":"2024-08-27T13:15:57.480268Z","iopub.status.idle":"2024-08-27T13:15:58.492003Z","shell.execute_reply.started":"2024-08-27T13:15:57.480225Z","shell.execute_reply":"2024-08-27T13:15:58.490843Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 5. Train the SimCLR Model","metadata":{}},{"cell_type":"code","source":"model = SimCLRModel()\ntrainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\ntrainer.fit(model, dataloader_train_simclr)\nprint('Pre-training is complete.')","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:16:23.942190Z","iopub.execute_input":"2024-08-27T13:16:23.942575Z","iopub.status.idle":"2024-08-27T13:47:25.901399Z","shell.execute_reply.started":"2024-08-27T13:16:23.942540Z","shell.execute_reply":"2024-08-27T13:47:25.900210Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (42) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a78b3b5cd2490bade9340f06272938"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Pre-training is complete.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 6. Save the Pre-trained Model","metadata":{}},{"cell_type":"code","source":"#You could use the pre-trained model and train a classifier on top.\npretrained_resnet_backbone = model.backbone\n\n#You can also store the backbone and use it in another code\nstate_dict = {\"resnet18_parameters\": pretrained_resnet_backbone.state_dict()}\ntorch.save(state_dict, \"simclr_garbage.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:52:06.626838Z","iopub.execute_input":"2024-08-27T13:52:06.627248Z","iopub.status.idle":"2024-08-27T13:52:06.691940Z","shell.execute_reply.started":"2024-08-27T13:52:06.627207Z","shell.execute_reply":"2024-08-27T13:52:06.690824Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 7. Perform Transfer Learning","metadata":{}},{"cell_type":"code","source":"model_1 = models.resnet18(pretrained=False)  # Load  ResNet-18\nnum_features = model_1.fc.in_features  # Get the number of input features for the last fully connected layer\nmodel_1.fc = nn.Linear(num_features, 12)  # Modify the output layer for your specific problem\n\nmodel_2 = models.resnet18(pretrained=True)  # Instantiate a new ResNet-18 model\nmodel_2.fc = nn.Linear(model_2.fc.in_features, 12)  # Modify the output layer for your specific problem\n# Load the saved SimCLR pretrained weights\npath_to_pretrained = 'simclr_garbage.pth'\nmodel_2.load_state_dict(torch.load(path_to_pretrained), strict=False)\n\n\n# Set the model2 to evaluation mode (important to prevent dropout, batch normalization, etc., from affecting forward pass)\nmodel_2.eval()\n\n\n# Transfer weights from model2 to model1, excluding the last layer (output layer)\nmodel_1_dict = model_1.state_dict()\nmodel_2_dict = model_2.state_dict()\n\n# Filter out unnecessary keys from model2_dict\nmodel_2_dict = {k: v for k, v in model_2_dict.items() if k in model_1_dict and 'fc' not in k}\n\n# Update model_1_dict with values from model_2_dict\nmodel_1_dict.update(model_2_dict)\n\n# Load the updated state_dict into model_1\nmodel_1.load_state_dict(model_1_dict)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:52:16.826301Z","iopub.execute_input":"2024-08-27T13:52:16.826805Z","iopub.status.idle":"2024-08-27T13:52:17.764568Z","shell.execute_reply.started":"2024-08-27T13:52:16.826755Z","shell.execute_reply":"2024-08-27T13:52:17.763491Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 148MB/s] \n/tmp/ipykernel_36/933397317.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_2.load_state_dict(torch.load(path_to_pretrained), strict=False)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 8. Retrain the Supervised Model with the Weights Obtained trough Pre-training","metadata":{}},{"cell_type":"code","source":"%%time\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_1.parameters(), lr=0.001)\n\nnum_epochs = 20  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint('Device: ', device)\nmodel_1.to(device)\n\nfor epoch in range(num_epochs):\n    model_1.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model_1(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {running_loss / len(train_loader)}\")\n\nprint(\"Training completed.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T13:52:25.760138Z","iopub.execute_input":"2024-08-27T13:52:25.760535Z","iopub.status.idle":"2024-08-27T14:01:01.603139Z","shell.execute_reply.started":"2024-08-27T13:52:25.760497Z","shell.execute_reply":"2024-08-27T14:01:01.602187Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Device:  cuda\nEpoch [1/20] Loss: 1.161772913792554\nEpoch [2/20] Loss: 0.7451676957163156\nEpoch [3/20] Loss: 0.5213552676871711\nEpoch [4/20] Loss: 0.44773638058526843\nEpoch [5/20] Loss: 0.30860307169895546\nEpoch [6/20] Loss: 0.24355591450105696\nEpoch [7/20] Loss: 0.24272805711656226\nEpoch [8/20] Loss: 0.2018017700380262\nEpoch [9/20] Loss: 0.17227554308506204\nEpoch [10/20] Loss: 0.12211893288893443\nEpoch [11/20] Loss: 0.13374521726669342\nEpoch [12/20] Loss: 0.08304232521000884\nEpoch [13/20] Loss: 0.04813294818022234\nEpoch [14/20] Loss: 0.08096590420737972\nEpoch [15/20] Loss: 0.14850555443405813\nEpoch [16/20] Loss: 0.0978841343255458\nEpoch [17/20] Loss: 0.11204620728762273\nEpoch [18/20] Loss: 0.06582138011478544\nEpoch [19/20] Loss: 0.027853412582201188\nEpoch [20/20] Loss: 0.03252619634640366\nTraining completed.\nCPU times: user 15min 5s, sys: 24.7 s, total: 15min 29s\nWall time: 8min 35s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 9. Test Again and Report Accuracy","metadata":{}},{"cell_type":"code","source":"model_1.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model_1(inputs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\naccuracy_after_transfer_learning = np.mean(np.array(all_preds) == np.array(all_labels))\nconf_matrix = confusion_matrix(all_labels, all_preds)\nclass_report = classification_report(all_labels, all_preds)\n\nprint(f\"Accuracy: {accuracy_after_transfer_learning:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:02:44.889745Z","iopub.execute_input":"2024-08-27T14:02:44.890476Z","iopub.status.idle":"2024-08-27T14:02:57.860964Z","shell.execute_reply.started":"2024-08-27T14:02:44.890435Z","shell.execute_reply":"2024-08-27T14:02:57.859981Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 0.8313\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10. Compare Accuracy Before and After Transfer Learning","metadata":{}},{"cell_type":"code","source":"print(f\"Accuracy before Transfer Learning: {(accuracy_before_transfer_learning)*100:.0f}%\")\n\nprint(f\"Accuracy After  Transfer Learning: {(accuracy_after_transfer_learning)*100:.0f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:17:12.396812Z","iopub.execute_input":"2024-08-27T14:17:12.397218Z","iopub.status.idle":"2024-08-27T14:17:12.402517Z","shell.execute_reply.started":"2024-08-27T14:17:12.397179Z","shell.execute_reply":"2024-08-27T14:17:12.401592Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Accuracy before Transfer Learning: 51%\nAccuracy After  Transfer Learning: 83%\n","output_type":"stream"}]},{"cell_type":"code","source":"improvement = (accuracy_before_transfer_learning/accuracy_after_transfer_learning)*100\nprint(f\" The classification accuracy has been improved by {improvement:.0f}% thanks to pre-training using SimCLR while keeping the size of the labeled data unchanged.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-27T14:18:51.742197Z","iopub.execute_input":"2024-08-27T14:18:51.742594Z","iopub.status.idle":"2024-08-27T14:18:51.748007Z","shell.execute_reply.started":"2024-08-27T14:18:51.742557Z","shell.execute_reply":"2024-08-27T14:18:51.747090Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":" The classification accuracy has been improved by 61% thanks to pre-training using SimCLR while keeping the size of the labeled data unchanged.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}